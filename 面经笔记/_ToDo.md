<!-- TOC -->

- [网络](#网络)
    - [阐述TCP连接四次挥手](#阐述tcp连接四次挥手)
        - [TCP包头的格式](#tcp包头的格式)
    - [为什么TCP建立连接需要三次握手，而断开连接需要四次挥手？](#为什么tcp建立连接需要三次握手而断开连接需要四次挥手)
        - [实现的功能](#实现的功能)
- [操作系统](#操作系统)
    - [进程间通讯](#进程间通讯)
    - [线程间通讯](#线程间通讯)
    - [进程和线程的区别](#进程和线程的区别)
    - [线程出现带来的便利](#线程出现带来的便利)
    - [如何查看一个进程打开了哪些文件？](#如何查看一个进程打开了哪些文件)
    - [shell](#shell)
    - [tcp慢启动算法](#tcp慢启动算法)
    - [Mysql 比较熟悉是吧？说一下底层数据存储原理？](#mysql-比较熟悉是吧说一下底层数据存储原理)
    - [数据库中的ACID](#数据库中的acid)
    - [产生死锁的四个必要条件](#产生死锁的四个必要条件)
    - [Python内置字典如何实现对value排序？](#python内置字典如何实现对value排序)
    - [http get post区别](#http-get-post区别)
    - [osi七层模型](#osi七层模型)
- [机器学习](#机器学习)
    - [特征](#特征)
    - [AUC](#auc)
    - [SVM](#svm)
    - [lr](#lr)
    - [树](#树)
    - [LDA与PCA联系与区别](#lda与pca联系与区别)
    - [决策树，XGBoost，随机森林处理缺失值的方法？](#决策树xgboost随机森林处理缺失值的方法)
        - [决策树如何处理缺失值？（三个阶段：划分节点时--训练模型时--预测时）](#决策树如何处理缺失值三个阶段划分节点时--训练模型时--预测时)
        - [XGBoost里处理缺失值的方法](#xgboost里处理缺失值的方法)
        - [随机森林处理缺失值的方法](#随机森林处理缺失值的方法)
    - [优化方法的区别？](#优化方法的区别)
- [CUDA](#cuda)
    - [内存结构](#内存结构)
        - [CUDA里面的内存，问你各个内存快慢，存放在哪里，应用场景是什么](#cuda里面的内存问你各个内存快慢存放在哪里应用场景是什么)
        - [避免寄存器溢出](#避免寄存器溢出)
        - [问: 如何查看内核所用的寄存器数量或共享/常量内存大小？](#问-如何查看内核所用的寄存器数量或共享常量内存大小)
    - [epoll](#epoll)

<!-- /TOC -->
# 网络
## 阐述TCP连接四次挥手
> CSDN/[TCP三次挥手和四次挥手](https://blog.csdn.net/qzcsu/article/details/72861891)
> [网友](http://www.vvbin.com/?p=707)
> 极客时间/[TCP](https://time.geekbang.org/column/article/8975)
- 请求->回应->回应回应
### TCP包头的格式
- 源端口号和目标端口号
- 包序号
- 包的确认序号
- 状态位：SYN 是发起一个连接，ACK 是回复，RST 是重新连接，FIN使结束连接
- 窗口大小

## 为什么TCP建立连接需要三次握手，而断开连接需要四次挥手？
因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。

### 实现的功能
丢包重传、乱序重排、校验数据、拥塞控制

# 操作系统
## 进程间通讯
常见的通信方式：
1. 管道pipe：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。
2. 命名管道FIFO：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。
3. 消息队列MessageQueue：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。
4. 共享存储SharedMemory：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。
5. 信号量Semaphore：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
6. 套接字Socket：套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同及其间的进程通信。
7. 信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。
- 辅助命令
- ipcs命令用于报告共享内存、信号量和消息队列信息。
- ipcs -a：列出共享内存、信号量和消息队列信息。
- ipcs -l：列出系统限额。
- ipcs -u：列出当前使用情况。

## 线程间通讯
互斥量、临界区、信号量和事件

## 进程和线程的区别
> [知乎](https://www.zhihu.com/question/44087187/answer/136188761)
（1）进程是资源的分配和调度的一个独立单元，而线程是CPU调度的基本单元（根本区别：进程是操作系统资源分配的基本单位，而线程是任务调度和执行的基本单位）
（2）同一个进程中可以包括多个线程，并且线程共享整个进程的资源（寄存器、堆栈、上下文），一个进程至少包括一个线程。
（3）进程的创建调用fork或者vfork，而线程的创建调用pthread_create，进程结束后它拥有的所有线程都将销毁，而线程的结束不会影响同个进程中的其他线程的结束
（4）线程是轻两级的进程，它的创建和销毁所需要的时间比进程小很多，所有操作系统中的执行功能都是创建线程去完成的
（5）线程中执行时一般都要进行同步和互斥，因为他们共享同一进程的所有资源
（6）线程有自己的私有属性TCB，线程id，寄存器、硬件上下文，而进程也有自己的私有属性进程控制块PCB，这些私有属性是不被共享的，用来标示一个进程或一个线程的标志

## 线程出现带来的便利
创建、终止、切换thread的开销要比process小的多
由于共享地址空间，线程通信比进程通信高效得多

## 如何查看一个进程打开了哪些文件？
lsof -P 18400 （按照进程ID查看）
lsof -d type (按照FD的类型查看）
lsof -i 4 (查看进程打开的网络连接，使用IPV4协议）
lsof -i :80 (查看进程打开的网络连接，端口号为80）
lsof -i @127.0.0.1 (查看进程打开的网络连接，IP为127.0.0.1）


## shell 
怎么实现对一个文件中的qq号出现次数统计？
A: 匹配腾讯QQ号：`^[1-9]*[1-9][0-9]*$`
Q: `grep -o '[1-9][0-9]*' QQ | sort | uniq -c| sort -r`  
Q: 如何打开一个100G的文件？
A: `split -l 100 abc.txt`
Q: 找出文本中包含某个词的行并且按照字典顺序排序？
A: `grep -nd xxx file` 

## tcp慢启动算法

## Mysql 比较熟悉是吧？说一下底层数据存储原理？
B+树，红黑树
1. 每个结点或者为黑色或者为红色。
2. 根结点为黑色。
3. 每个叶结点(实际上就是NULL指针)都是黑色的。
4. 如果一个结点是红色的，那么它的两个子节点都是黑色的（也就是说，不能有两个相邻的红色结点）。
5. 对于每个结点，从该结点到其所有子孙叶结点的路径中所包含的黑色结点数量必须相同。

平衡指的是树的左右子树的高度差距在一个可控的范围内。图中红黑树也是平衡的，不过红黑树的平衡标准比avl树低。avl树要求左右子树高度差不大于1。而红黑树中红节点的父亲和孩子必须是黑节点，且从根到叶子节点经过的黑节点个数相同，因此红黑树最小深度是路径上只有黑节点，最大深度是路径上红黑节点相互间隔，因此最大深度≤最小深度的两倍，最大深度是2*log2（n+1）。因此，红黑树的查询效率比avl树低，但是红黑树的删除效率比avl树高，更适合大量数据增加删除的场景，而且红黑树在增加删除数据的时候只需要常数次旋转操作，更适合数据持久化的场景。

## 数据库中的ACID
> [csdn](https://blog.csdn.net/corbin_zhang/article/details/80578005)
Atomicity 原子性： 事务是一个不可再分割的工作单元
Consisitency 一致性 执行一个事务前和后，数据库的完整性约束没有没有被破坏。
Isolation 隔离性 每个事务应该是隔离的
Durability 持久性 持久性意味着事务执行完成后，该事务对数据库的更改便持久到了数据库中，这个更改是永久的。

 1. 脏读：事务A读取了事务B的更新的数据，但是事务B回滚了，导致A读取的为脏数据。

 2. 不可重复读：事务A读取同一数据两次，但是在两次之间事务B对该数据进行了修改并提交，导致事务A读取两次读取不一致

 3. 幻读：事务A修改全表的数据，在未提交时，事务B向表中插入或删除数据，导致事务A读取的数据与需要修改的数据不一致，就和幻觉一样。
注意：不可重复读和幻读很容易混淆，不可重复读针对的是数据的修改，幻读针对的时数据的新增和删除。解决不可重复读问题只需要给对应记录上行锁，而解决幻读需要对表加锁。

隔离级别

 1. 未提交读（read uncommitted），就是不做隔离控制，可以读到“脏数据”
 2. 提交读（read committed），提交读就是不允许读取事务没有提交的数据。显然这种级别可以避免了脏读问题
 3. 可重复读（repeatable read），与提交读（不可重复读）相对应，为了避免提交读级别不可重复读的问题，在事务中对符合条件的记录上排他锁，这样其他事务不能对该事务操作的数据进行修改，可避免不可重复读的问题产生
 4. 序列化（Serializable），在事务中对表上锁，这样在事务结束前，其他事务都不能够对表数据进行操作（包括新增，删除和修改），这样避免了脏读，不可重复读和幻读，

## 产生死锁的四个必要条件
（1） 互斥条件：一个资源每次只能被一个进程使用。
（2） 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
（3） 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。
（4） 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。

## Python内置字典如何实现对value排序？

## http get post区别
post 的body可以被抓包，怎么防止？  改用HTTPs？
https怎么实现的？     拉闸，不会
arp协议全过程

## osi七层模型
七层结构：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层
tcp属于传输层，http是应用层
七层:
1. 应用层   五层奖前三层变成应用层      
2. 表示层        
3. 会话层
4. 传输层
5.  网络层
6. 数据链路层
7 .物理层

# 机器学习
## 特征
特征为什么要归一化？归一化的三种方式？归一化后还要做什么？
特征选择的方法？

## AUC
ROC曲线下的面积，横坐标为false positive, 纵坐标为true positive

## SVM
- SVM为什么要从原始问题转变为对偶问题
1. 降低求解复杂度，原问题的最后求解是对w,b进行求解，其维数为训练样本的特征维数+1，对偶问题最后是对 $\alpha$ 进行求解，其维数（个数）等于样本的个数，所以svm对于高维空间中较稀疏的样本表现较好
2. 方便核函数的引入

## lr
- 逻辑回归原理
- lr优缺点
- lr与SVM的区别和联系
```
联系：
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）

2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。

3、LR和SVM都可以用来做非线性分类，只要加核函数就好。

4、LR和SVM都是线性模型，当然这里我们不要考虑核函数

5、都属于判别模型

区别：

1、LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。

2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。

3、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。

4、SVM依赖penalty系数，实验中需要做CV

5、SVM本身是结构风险最小化模型，而LR是经验风险最小化模型

那怎么根据特征数量和样本量来选择SVM和LR模型呢？Andrew NG的课程中给出了以下建议：
如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。(LR和不带核函数的SVM比较类似。)

```
逻辑回归SGD步长不对，收敛出现问题，该怎么解决？（减小学习率或者增大Batch Size）
逻辑回归模型里面有时一直收敛不到阈值以下的参数，该怎么做？
L1和L2的区别
手写逻辑回归目标函数，目标函数对w求导，手写。基础题。

## 树
树模型对归一化不敏感。
- **树模型的优势**？
- 树模型（非线性模型）和线性模型的区别？为什么会对离散、连续特征有统一的处理？
- 随机森林怎么才能学习出特征和特征之间的关系？
- GBDT、XGBoost、和RF的区别
```
GBDT和RF
1. 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成，GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）
2. 组成随机森林的树可以并行生成；而GBDT只能是串行生成 
3. 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来 
4. 随机森林对异常值不敏感，GBDT对异常值非常敏感 
5. 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成 
6. 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能

GBDT与XGBoost
1. xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性
2. xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
AdaBoost 相对于 bagging 算法和 Random Forest 算法，AdaBoost 充分考虑的每个分类器的权重。
3. xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性

1. xgboost在目标函数中加入了正则化项，当正则化项为0时与传统的GDBT的目标函数相同2. xgboost在迭代优化的时候使用了目标函数的泰勒展开的二阶近似，paper中说能加快优化的过程！！xgboost可自定义目标函数，但是目标函数必须二阶可导也是因为这个。GDBT中只用了一阶导数。3. xgboost寻找最佳分割点时，考虑到传统贪心法效率比较低，实现了一种近似贪心法，除此之外还考虑了稀疏数据集、缺失值的处理，这能大大提升算法的效率。paper中提到在一个稀疏数据集中测试，发现速度提升了50倍。4. xgboost在算法实现时做了很多优化，大大提升了算法的效率，感叹陈天奇大牛深厚计算机基础！对训练的每个特征排序并且以块的的结构存储在内存中，方便后面迭代重复使用，减少计算量，不仅如此，在不同的特征属性上采用多线程并行方式寻找最佳分割点上述的优化导致每个样本的梯度信息在内存中不连续，直接累加有可能会导致cache-miss，所以xgboost先将样本的统计信息取到线程的内部buffer，然后再进行小批量的累加xgboost在实现时考虑了当训练数据很大、内存空间不够时，如何有效的利用磁盘空间？主要是利用了分块、预取、压缩、多线程协作的思想。

lightGBM

4.1 与XGboost对比

　　1、xgboost采用的是level-wise的分裂策略，而lightGBM采用了leaf-wise的策略，区别是xgboost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是xgboost也进行了分裂，带来了额外的开销。 leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。

　　2、lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法，histogram算法在内存和计算代价上都有不小优势。

　　（1）内存上优势：很明显，直方图算法的内存消耗为(#data*#features * 1Bytes)(因为对特征分桶后只需保存特征离散化之后的值)，而xgboost的exact算法内存消耗为：(2 * #data * #features* 4Bytes)，因为xgboost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。

　　（2）计算上的优势，预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为(#data),而直方图算法只需要遍历桶就行了，时间为(#bin)

　　3、直方图做差加速

一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。

　　4、lightgbm支持直接输入categorical 的feature

在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。

　　5、实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？

-. xgboost在每一层都动态构建直方图，因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。
```
- 树的深度、树的棵树是怎么调的？
- 信息增益、信息增益率、基尼系数三个公式

## LDA与PCA联系与区别
首先我们看看相同点：

1）两者均可以对数据进行降维。

2）两者在降维时均使用了矩阵特征分解的思想。

3）两者都假设数据符合高斯分布。

我们接着看看不同点：

1）LDA是有监督的降维方法，而PCA是无监督的降维方法

2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。

3）LDA除了可以用于降维，还可以用于分类。

4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。

## 决策树，XGBoost，随机森林处理缺失值的方法？
### 决策树如何处理缺失值？（三个阶段：划分节点时--训练模型时--预测时）
1. 在选择分裂属性的时候，训练样本存在缺失值，如何处理？假如你使用ID3算法，那么选择分类属性时，就要计算所有属性的熵增(信息增益，Gain)。假设10个样本，属性是a,b,c。在计算a属性熵时发现，第10个样本的a属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算a属性的熵增。然后结果乘0.9（新样本占raw样本的比例），就是a属性最终的熵。
2. 分类属性选择完成，对训练样本分类，发现属性缺失怎么办？比如该节点是根据a属性划分，但是待分类样本a属性缺失，怎么办呢？假设a属性离散，有1,2两种取值，那么就把该样本分配到两个子节点中去，但是权重由1变为相应离散值个数占样本的比例。然后计算错误率的时候，注意，不是每个样本都是权重为1，存在分数。
3. 训练完成，给测试集样本分类，有缺失值怎么办？这时候，就不能按比例分配了，因为你必须给该样本一个确定的label，而不是薛定谔的label。这时候根据投票来确定，或者填充缺失值。

### XGBoost里处理缺失值的方法
在xgboost里，在每个结点上都会将对应变量是缺失值的数据往左右分支各导流一次，然后计算两种导流方案对Objective的影响，最后认为对Objective降低更明显的方向（左或者右）就是缺失数据应该流向的方向，在预测时在这个结点上将同样变量有缺失值的数据都导向训练出来的方向。
例如，某个结点上的判断条件是 A>0 ，有些数据是A0，有些数据的A是缺失值。那么算法首先忽略带缺失值的数据，像正常情况下一样：

1. 将前两种数据分别计算并导流到左子树与右子树，
2. 然后将带缺失值的数据导向左子树，计算出这时候模型的Objective_L；
3. 接着将带缺失值的数据导向右子树，计算出这时候模型的Objective_R；
4. 最后比较Objective_L和Objective_R。
5. 假设Objective_L更小，那么在预测时所有变量A是缺失值的数据在这个结点上都会被导向左边，当作A<=0处理。

### 随机森林处理缺失值的方法
RandomForest包里有两种补全缺失值的方法：

方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。

方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似。

补充【proximity matrix】：Proximity 用来衡量两个样本之间的相似性。原理就是如果两个样本落在树的同一个叶子节点的次数越多，则这两个样本的相似度越高。当一棵树生成后，让数据集通过这棵树，落在同一个叶子节点的”样本对(xi,敏感词roximity 值 P(i,j)加 1。所有的树生成之后，利用树的数量来归一化proximity matrix。

## 优化方法的区别？
SGD、BGD等 区别 
- SGD
  - 每次读入一个数据，更新
  - 在线更新
  - 容易收敛到局部最低点和鞍点
- BGD
  - 所有数据更新
  - 凸函数能保证收敛，消耗内存大
- MBGD
  - 主流
- 为什么用MBGD？
  - 我们假设每个样本相对于大自然真实分布的标准差为σ，那么根据概率统计的知识，很容易推出n个样本的标准差为 $\sigma/\sqrt{n}$ （有疑问的同学快翻开概率统计的课本看一下推导过程）。从这里可以看出，我们使用样本来估计梯度的时候，1个样本带来σ的标准差，但是使用n个样本区估计梯度并不能让标准差线性降低（也就是并不能让误差降低为原来的1/n，即无法达到σ/n），而n个样本的计算量却是线性的（每个样本都要平等的跑一遍前向算法）。
  - > [训练神经网络时如何确定batch size？](https://zhuanlan.zhihu.com/p/27763696)
- 以上三种都需要调整学习率
- 动量法解决鞍点问题
- Adagrad/RMSprop/Adam
- 对于强大的二阶优化算法如共轭梯度法、L-BFGS来说，如果估计不好一阶导数，那么对二阶导数的估计会有更大的误差，这对于这些算法来说是致命的。
    - 对于二阶优化算法，减小batch换来的收敛速度提升远不如引入大量噪声导致的性能下降，因此在使用二阶优化算法时，往往要采用大batch哦。此时往往batch设置成几千甚至一两万才能发挥出最佳性能。

# CUDA
## 内存结构
### CUDA里面的内存，问你各个内存快慢，存放在哪里，应用场景是什么
> face2ai/[内存](https://www.face2ai.com/CUDA-F-4-1-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/)   

- 寄存器(registers)
- Local memory
    - 高延迟、低带宽
    - 本地内存存储在每个SM的一级缓存，或者设备的二级缓存
- 共享内存（shared memory）
  - 共享内存是片上内存，跟主存相比，速度要快很多，也即是延迟低，带宽高。其类似于一级缓存，但是可以被编程。
  - 延迟低，带宽高
  - 共享内存的时候一定要注意，不要因为过度使用共享内存，而导致SM上活跃的线程束减少，也就是说，一个线程块使用的共享内存过多，导致更过的线程块没办法被SM启动，这样影响活跃的线程束数量
  - SM中的一级缓存，和共享内存共享一个64k的片上内存（不知道现在的设备有没有提高），他们通过静态划分，划分彼此的容量，运行时可以通过下面语句进行设置：
- 全局内存（global memory）
  - 全局内存访问是对齐，也就是一次要读取指定大小（32，64，128）整数倍字节的内存
- 常量内存（constant memory）
  - 设备内存
  - 常量内存的读取机制是：一次读取会广播给所有线程束内的线程。
- 纹理内存（texture memory）
  - 纹理内存驻留在设备内存中，在每个SM的只读缓存中缓存，纹理内存是通过指定的缓存访问的全局内存，只读缓存包括硬件滤波的支持，它可以将浮点插入作为读取过程中的一部分来执行，纹理内存是对二维空间局部性的优化。总的来说纹理内存设计目的应该是为了GPU本职工作显示设计的，但是对于某些特定的程序可能效果更好，比如需要滤波的程序，可以直接通过硬件完成。

### 避免寄存器溢出
添加__lauch_bounds__关键字
```c++
__global__ void
__lauch_bounds__(maxThreadaPerBlock,minBlocksPerMultiprocessor)
kernel(...) {
    /* kernel code */
```

### 问: 如何查看内核所用的寄存器数量或共享/常量内存大小？ 
在 nvcc 命令行中添加「--ptxas-options=-v」选项即可。在编译过程中，此信息将输出至控制台。

## epoll
> [csdn](https://blog.csdn.net/davidsguo008/article/details/73556811)
 都用于I/O复用
 select 和 poll 类似
 （1）数量
 （2）每次调用都得传入fd集合
 （3）在内核遍历所有fd，观察fd是否发生

（1） epoll_create 创建epoll对象
（2） 调用epoll_ctl向epoll对象中添加100万个链接的套字
（3） 调用epoll_wait收集发生的事件的连接


## 从输入URL导页面加载的全过程
1. DNS解析（域名转IP）
2. TCP链接
3. 生成http请求，服务器处理并返回http报文
4. 浏览器解析渲染页面

## 递归查询
1. 本地服务器


## http1.0 http1.1 http2.0 https
http1.0 短链接
http1.1 长连接
http2.0 复用
头压缩、分帧、二进制编码
HTTPS 加密
1. c->s 客户端发送加密通信请求
2. 网站回传CA
     ca包括公钥，证书所有者，发布机构，有效期
    CA的作用，是保证服务器的公钥的来历，其做法是对公钥实行哈希摘要算法，然后用CA私钥加密，伴随公钥一起发出去。
    客户端收到后，用CA公钥解密，然后对公钥做哈希，比对哈希值是否一致。
3. 